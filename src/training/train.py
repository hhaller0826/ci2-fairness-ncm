import torch as T
import torch.optim as optim
from torch.utils.data._utils.collate import default_collate

from src.metric.divergence import *

def train_ncm(model, data, optimizer=None, hyperparameters={}):
    device = hyperparameters.get('device', 'cpu')
    lr = hyperparameters.get('learning-rate', 1e-3)
    num_epochs = hyperparameters.get('n-epochs', 10)

    dataloader = data.train_dataloader

    model.to(device)
    model.train()
    ordered_v = model.v

    if optimizer == None: optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(1, num_epochs+1):
        epoch_loss = 0.0
        for batch in dataloader:
            # if DataLoader gives you back a list of samples, collate it
            if isinstance(batch, list):
                batch = default_collate(batch)

            # now batch is a dict of batched tensors
            # Batch = real data (so the data generated by M*)
            batch = {k: v.to(device) for k, v in batch.items()}
            batch_size = next(iter(batch.values())).shape[0]
            # Data generated by \hat{M}
            ncm_batch = model(batch_size)

            # convert them into matrices
            data_matrix = T.cat([batch[k] for k in ordered_v], axis=1)
            ncm_matrix = T.cat([ncm_batch[k] for k in ordered_v], axis=1)

            optimizer.zero_grad()
            # MMD loss compares the probability distributions of the two matrices
            loss = MMD_loss(data_matrix.float(),ncm_matrix,gamma=1)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(dataloader)
        print(f"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}")

    return model

def compute_accuracy(model, dataloader, device, target_var):
    model.eval()
    total_loss = 0
    total_e = 0
    total_js = 0
    with T.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            batch_size = next(iter(batch.values())).shape[0]

            preds=model(n=batch_size,select={target_var})[target_var]
            labels = batch[target_var]
            
            labels = T.cat([batch[target_var]], axis=1)
            pred_labels = T.cat([preds], axis=1)
            total_loss += MMD_loss(labels,pred_labels)

            total_e += energy_distance(labels,pred_labels)
            total_js += classifier_js_divergence(labels,pred_labels)

    print(f'\t\t energy-based: {1-(total_e/len(dataloader)):.4f}')
    print(f'\t\t js-divergence: {1-(total_js/len(dataloader)):.4f}')
    return 1-(total_loss/len(dataloader))

def print_accuracy(var, trained_ncm, train_dataloader, test_dataloader):
    train_acc = compute_accuracy(trained_ncm, train_dataloader, 'cpu', var)
    print(f'Final train accuracy for {var}: {train_acc:.4f}')

    test_acc = compute_accuracy(trained_ncm, test_dataloader, 'cpu', var)
    print(f'Final test accuracy  for {var}: {test_acc:.4f}')

