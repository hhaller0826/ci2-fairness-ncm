{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhaller0826/ci2-fairness-ncm/blob/main/IntroFairnessNCM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgunRRxRSECt"
      },
      "source": [
        "# Introduction/Explanation\n",
        "\n",
        "Causal Inference [***TODO***: briefly explain the field of study & purpose of the notebook, especially as it relates to non-causal researchers]\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "**Section 1: Create a Causal Graph** \n",
        "This will walk you through creating a graph to represent relationships between the variables you plan to analyze. \n",
        "\n",
        "**Section 2: Train the Model**\n",
        "This will walk you through creating a neural causal model, and training it to learn the relationships that are represented in your causal graph based on your data.\n",
        "\n",
        "**Section 3: Extract Causal Insights from the Model**\n",
        "This will provide instructions for extracting information about the causal relationships in your trained model. This may contain useful tools for your analysis, but does not contain any steps that are necessary for progressing to Section 4.\n",
        "\n",
        "**Section 4: Project onto the Standard Fairness Model**\n",
        "This will walk you through projecting your trained model onto a standard fairness model, which can then be used to perform fairness analysis. Note that the tools for extracting metrics described in Section 3 can also be applied to the resulting standard fairness model. \n",
        "\n",
        "**Section 5: Run Fairness Tasks** \n",
        "This will walk you through using your Standard Fairness Model projection to perform bias detection, fair prediction, and fair decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell before you progress to ensure this notebook has access to the necessary code.\n",
        "from src.graph.default_graphs import *\n",
        "from src.graph.utils import *\n",
        "from trashfiles.dummy_functions import *\n",
        "from src.causalaibook.fusion import * "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj1bp-5ESFoq"
      },
      "source": [
        "# Create a Causal Graph\n",
        "\n",
        "A causal graph (or \"causal diagram\") represents the relationships between the features in your data. Each node on the graph may represent one or more of these features. Please note that if a single node represents multiple features, we will not be able to distinguish between these features when doing causal analysis. \n",
        "\n",
        "If the values in one node may impact the values in another node, the graph will have a directed edge from the first node to the one that it affects. \n",
        "\n",
        "Sometimes there are confounders between two variables (lets call them A and B), meaning they are both directly impacted by the same third variable C. If C is one of the nodes on your graph, then there will be one directed edge from C to A, and another from C to B. If C is *not* one of the nodes on your graph, then we will create a bi-directed edge between A and B to signify that they are confounded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples of some graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: add examples and plot them\n",
        "# TODO: express how maybe X = ['race','age'] OR X1 = 'race' and X2 = 'age' \n",
        "# TODO: express how you don't need an arrow for indirect effects. Like if A-->B-->C we may not need an arrow A-->C.\n",
        "\n",
        "# I am tryna give them the bare minimum of what they need to know to implement this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define your graph below.\n",
        "\n",
        "You can use one of the pre-defined graphs, like so:\n",
        "```\n",
        "my_graph = get_predefined_graph(type='____')\n",
        "```\n",
        "Valid types include 'bow', 'backdoor', etc. ***TODO***\n",
        "\n",
        "\n",
        "Alternatively, you create your own graph. Here is an example:\n",
        "```\n",
        "nodes = ['W', 'X', 'Y', 'Z']\n",
        "edges = [('X', 'Y'),\n",
        "        ('X', 'W'),\n",
        "        ('Z', 'Y'),\n",
        "        ('Z', 'W'),\n",
        "        ('W', 'Y'),\n",
        "        ('X', 'Z', 'bidirected'),\n",
        "        ('Z', 'Y', 'bidirected')]\n",
        "\n",
        "my_graph = CausalGraph(nodes=nodes, edges=edges)\n",
        "```\n",
        "\n",
        "*Remember*: when defining the directed, the arrow goes from the first node to the second one. So for example, ```edges = [('X', 'Y')]``` indicates one edge going from 'X' to 'Y'. When defining a bidirected edge, it does not matter which direction nodes are listed, but you must add the key term `'bidirected'` after listing the two nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using a pre-defined graph:\n",
        "my_graph = parseGraph(get_predefined_graph('backdoor'))\n",
        "\n",
        "# using a custom graph: \n",
        "nodes = ['A', 'B', 'C', 'D', 'E']\n",
        "edges = [\n",
        "    ('C', 'A', 'bidirected'),\n",
        "    ('C', 'E'),\n",
        "    ('C', 'D'),\n",
        "    ('A', 'B'),\n",
        "    ('A','E'),\n",
        "    ('B','D'),\n",
        "    ('B','E'),\n",
        "    ('D','E')\n",
        "]\n",
        "\n",
        "my_graph = CausalGraph(nodes=nodes, edges=edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Double-check that this is the graph you want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_causal_graph(my_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTCqyhX8S4s4"
      },
      "source": [
        "# Train the Model\n",
        "Explain NCM (model that learns the relationships between the variables in your graph based on your data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete the Structural Causal Model\n",
        "Now that you have defined the graph we have the variables and dependencies, but we still need to define ___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = process_data('path_to_data')\n",
        "\n",
        "# This will define how we determine the probabilities for variables whose values \n",
        "# were not observed/recorded. \n",
        "distribution = 'probability distribution for exogenous variables'\n",
        "\n",
        "# Now define which columns of your data correspond to each node in your graph:\n",
        "assignments = {\n",
        "    'A': ['race'],\n",
        "    'B': ['skincolor'],\n",
        "    'C': ['gender'],\n",
        "    'D': ['job_title', 'degree', 'num_awards'],\n",
        "    'E': ['salary']\n",
        "}\n",
        "\n",
        "my_model = Model(data=data, distribution=distribution, graph=CausalGraph(my_graph,assignments=assignments), assignments=assignments)\n",
        "### NOTE for reviewers: if the assignment contains a feature which is not in the data\n",
        "# or if there is a duplicate of the same feature, this will throw an error. \n",
        "# If there is a feature in the data that was not assigned to any variable it will output\n",
        "# a warning but otherwise be fine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When training the network, you are able to manually adjust various settings called \"hyperparameters\". These include the following:\n",
        "* `pipeline_choice`: this specifies the type of neural network that will be ran. The options are as follows,\n",
        "    * `'mle'`: Maximum Likelihood Estimator\n",
        "    * `'gan'`: Generative Adversarial Network\n",
        "    * `'ffn'`: Feed Forward Network\n",
        "    The default is 'mle'. \n",
        "* `n-epochs`: the number of epochs, or passes through the dataset to complete during the training phase, to run. Default: 1000. \n",
        "* `batch-size`: number of training examples to use in each training epoch. \n",
        "* `gpu`: the GPU to use, if you have one. Default: none\n",
        "\n",
        "You may set some, none, or all of these values manually. Valid definitions include:\n",
        "```\n",
        "hyperparameters = {\n",
        "    'pipeline_choice': 'gan',\n",
        "    'n-epochs': 500\n",
        "}\n",
        "```\n",
        "```\n",
        "hyperparameters = {}\n",
        "```\n",
        "```\n",
        "hyperparameters = {\n",
        "    'pipeline_choice': 'ffn',\n",
        "    'n-epochs': 4600,\n",
        "    'batch-size': 4,\n",
        "    'gpu': gpu\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your parameters here:\n",
        "hyperparameters = {\n",
        "    'pipeline_choice': 'gan'\n",
        "}\n",
        "\n",
        "# And train the model:\n",
        "run_training(my_graph, my_model, hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYOhC9bCTRMh"
      },
      "source": [
        "# Extract Causal Insights from the Model\n",
        "Now that you have a trained model, you can use it to evaluate cause-effect relationships in your data. The metrics that you can evaluate are split into three categories: (1) values that can be computed directly from the data, (2) values that would occur if you force a variable to take on a certain value, and (3) questions about would have occurred if things had been different (I need to reword that so bad)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observational Understanding\n",
        "Observational metrics can be calculated directly from the data. These include calculating standard probabilities and conditional probabilities (ex.: $P(Y=1)$ or $P(Y=1|X=0)$), as well as something called the \"total variation\".\n",
        "\n",
        "*Total Variation (TV)*: a measure of how much the distribution of some variable *Y* is impacted by the value of a given attribute X.\n",
        "$$TV_{x_0,x_1}(Y) = P(Y|X=x_1)-P(Y|X=x_0)$$\n",
        "\n",
        "The available observational measures are defined as follows:\n",
        "* `probability(variable, value, evidence)`: This will calculate the probability that a variable equals a certain value given the dictionary of evidence. The dictionary of conditional evidence is optional.\n",
        "    * `probability('Y',1)` will return $P(Y=1)$\n",
        "    * `probability('Y',1,{'X':0,'Z':2})` will return $P(Y=1|X=0,Z=2)$\n",
        "* `total_variation(variable, value, attr, aval0, aval1)` will return the total variation of the given variable relative to the given attribute. $TV_{aval0,aval1}(variable=value)$. `avail1` is an optional value, and will default to \"not avail0\" if left blank. \n",
        "    * `total_variation('Y',1,'X',x0,x1)` will return $TV_{x_0,x_1}(Y=1) = P(Y=1|X=x_1)-P(Y=1|X=x_0)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_variation('Y', 1, 'X', 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interventional Metrics\n",
        "Interventional metrics arise when you actively say \"what if we force this variable to take on a certain value?\"\n",
        "The difference between a conditional probability ($P(Y|X=x)$) and an interventional one ($P(Y|do(X=x))$) is that when you force an event to happen, you emancipate it from all other influences. [kinda stole this wording from the book of why but not exactly...]\n",
        "\n",
        "*For example*: say that patients who go to the doctor for a certain illness all recieve advice and prescription medication, and this allows them to heal within a week. Say that those who have not recieved the advice nor the medication don't heal so quickly.\n",
        "\n",
        "![](img/dr_advice_meds_healed.png)\n",
        "\n",
        "If we are trying to evaluate the likelihood that someone has healed, and we observe that they have recieved the advice, we can assume from our data that they went to the doctor and recieved medication too. \n",
        "\n",
        "Essentially, {patient got advice} $\\implies$ {patient went to doctor} $\\implies$ {patient got advice} AND {patient got medication}\n",
        "\n",
        "This means that $P(\\text{healed} | \\text{advice})=P(\\text{healed} | \\text{advice},\\text{medication})$.\n",
        "\n",
        "But what if a patient hears the doctor's advice from a friend? In this case, even though the patient has heard the advice, they may not have seen the doctor, and they may not have access to the medication. This is an example of an intervention. Although we can usually assume that someone who has heard the advice has seen a doctor, the friend intervened and gave the patient advice, so even though we know the patient has heard the advice, we don't have any knowledge of whether or not they went to a doctor and recieved medication. Here, $P(\\text{healed} | do(\\text{advice}))\\neq P(\\text{healed} | \\text{advice})$.\n",
        "\n",
        "*Total Effect (TE)*:\n",
        "\n",
        "*z-Total Effect (z-TE)*:\n",
        "\n",
        "The available interventional measures are defined as follows:\n",
        "* `probability(variable, value, evidence, intervention)`: This will calculate the probability that a variable equals a certain value given the dictionary of evidence, and the dictionary of interventions. Both dictionaries are optional.\n",
        "    * `probability('Y',1,{'X':0},{'Z':1})` will return $P(Y=1 | X=0, do(Z=1))$\n",
        "    * `probability('Y',1,intervention={'Z':1})` will return $P(Y=1 | do(Z=1))$\n",
        "* `total_effect('Y', e=evid, c=cond)`\n",
        "* z-TE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evid = [\n",
        "    ('A', 1),\n",
        "    ('B', 2)\n",
        "]\n",
        "cond = [\n",
        "    ('C', 3)\n",
        "]\n",
        "total_effect('Y', e=evid, c=cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What-ifs [rename]\n",
        "Lastly we can evaluate \"counterfactuals\", named because it is the probability of something that we have no real world data on. \n",
        "\n",
        "For example, say people have the option to take a medication or heal naturally, and that the observational data shows that individuals who healed naturally actually survived longer on average. Is this because the medication was harmful, or is it because the *types* of people who decided to heal naturally were typically younger, healthier, had fewer comorbidities, or had fewer symptoms to begin with? \n",
        "\n",
        "What is the probability that someone who opted out of medication would have healed had they taken it? What is the probability that someone who decided to take medication would have healed if they never got it? \n",
        "\n",
        "These are \"counterfactual\" questions because to answer them we would have to go back in time and change history. There is no experiment that can deny treatment to an already treated person and compare the two outcomes. [book of why wording]\n",
        "\n",
        "***TODO:*** Explain $Y_X|X$ notation\n",
        "\n",
        "*Effect of Treatment on the Treated (ETT)*: evaluate whether people who gained access to treatment would have had the same outcome if they had never recieved it.\n",
        "$$ETT_{x,x'}(Y)=$$\n",
        "\n",
        "*Probability of Necessity (PN)*: encodes how much the presence of a value $X$ was a *necessary* cause to make $Y=1$.\n",
        "$$PN(X;Y)=P(Y_{X=0}=0|X=1,Y=1)$$\n",
        "\n",
        "*Probability of Sufficiency (PS)*: encodes how much the presence of a value $X$ was *sufficient* to make $Y=1$.\n",
        "$$PS(X;Y)=P(Y_{X=1}=1|X=0,Y=0)$$\n",
        "\n",
        "Both of the above translate to \"in a situation where $X=x$ and $Y=y$, what is the probability that the value of $Y$ would have been different if you had changed the value of $X$?\" These can be combined into \n",
        "$$PN/PS$$\n",
        "\n",
        "\n",
        "\n",
        "The available counterfactual measures are defined as follows:\n",
        "* `ett(outcome_var, outcome_val, treatment_var, treatment_vals={'actual':_, 'whatif':_})`: This will calculate the probability of a given outcome if we had intervened to change the treatment. In the `treatment_vals` dictionary, you must specify either `actual` or `whatif`, but you do not need to specify both. \n",
        "  * `ett('Y',1,'X',{'actual':0, 'whatif':1})` will return $P(Y_{X=1}=1|X=0)$, which is the probability that $Y=1$ if we had intervened to make $X=1$, given that $X$ was actually 0.\n",
        "  * `ett('Y',1,'X',{'whatif':1})` will return $P(Y_{X=1}=1|X\\neq 1)$, which is the probability that $Y=1$ if we had intervened to make $X=1$, given that $X$ was not actually 1.\n",
        "  * `ett('Y',1,'X',{'actual':0})` will return $P(Y_{X\\neq 0}=1|X=0)$, which is the probability that $Y=1$ if we had intervened to make $X\\neq 0$, given that $X$ was actually 0.\n",
        "\n",
        "In subsequent code there will be more examples where you are expected to provide a dictionary of the form `{'actual':_, 'whatif':_}`. Assume they all function like the one above, where you can specify one or both of those fields, and if you only specify one, then the other one will automatically be set to \"not that\". \n",
        "***Edit? This seems too complicated an explanation but do we want this flexibility?***\n",
        "\n",
        "* `pnps(outcome_var, outcome_vals={'actual':_, 'whatif':_}, treatment_var, treatment_vals={'actual':_, 'whatif':_})` \n",
        "  * `pnps('Y', outcome_vals={'actual':0, 'whatif':1}, 'X', treatment_vals={'actual':0, 'whatif':1})` will return the probability that $Y=1$ if we had intervened to make $X=1$, given that $Y$ and $X$ were actually both 0. \n",
        "* PNS\n",
        "* DE\n",
        "* IE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'P(Y_{X=1} = 1|X=0, Y=0)'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outcome_vals = {\n",
        "    'actual': 0,\n",
        "    'whatif': 1\n",
        "}\n",
        "treatment_vals = {\n",
        "    'actual': 0,\n",
        "    'whatif': 1\n",
        "}\n",
        "\n",
        "pnps('Y',outcome_vals,'X',treatment_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tv0OBYGTF5U"
      },
      "source": [
        "# Project onto the Standard Fairness Model\n",
        "\n",
        "The standard fairness model (SFM) is a model whose graph has four nodes:\n",
        "* *X*: The protected attribute. This ***fill in explanation***\n",
        "* *Z*: The counfounding variables. This ***fill in explanation***\n",
        "* *W*: The mediator variables. This ***fill in explanation***\n",
        "* *Y*: The outcome variable. This ***fill in explanation***\n",
        "\n",
        "When projecting onto the SFM, you should select one of your model's variables to be *X* and one to be *Y*, but you may assign multiple variables as confounders or mediators. Counfounders may have any relationship to the other confounders, and mediators may have any relationship with the other mediators, however there is a specific structure that must exist between *X*, *Z*, *W*, and *Y*:\n",
        "* *Y* must be a variable that has 0 arrows pointing toward *X*, *Z*, or *W*.\n",
        "* *W* must contain variables that have 0 arrows pointing toward *X* or *Z*.\n",
        "* *X* and *Z* cannot have arrows pointing towards each other, but they may have a bidirected arrow between them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Here is an example**:\n",
        "Given the following graph. \n",
        "\n",
        "![example1graph.png](img/example1graph.png)\n",
        "\n",
        "Based on this graph, you could do either of the following projections:\n",
        "```\n",
        "projection1 = {\n",
        "    'X': 'e',\n",
        "    'Z': ['a', 'b'],\n",
        "    'W': ['c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "\n",
        "or \n",
        "```\n",
        "projection2 = {\n",
        "    'X': 'a',\n",
        "    'Z': ['e'],\n",
        "    'W': ['b', 'c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "\n",
        "![](img/ex1projection1.png) ![](img/ex1projection2.png)\n",
        "\n",
        "The following would **not** be a valid projection: \n",
        "```\n",
        "bad_projection = {\n",
        "    'X': 'a',\n",
        "    'Z': ['b', 'e'],\n",
        "    'W': ['c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "because there is an arrow from the `X` variable (containing `'a'`) to the `Z` variable (containing `'b'`). This indicates that `X` may actually cause `Z`, rather than just being confounded with `Z`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify your projection here:\n",
        "projection = {\n",
        "    'X': 'C',\n",
        "    'Z': ['A', 'B'],\n",
        "    'W': ['D'],\n",
        "    'Y': 'E'\n",
        "}\n",
        "sfm = project_to_sfm(my_model.cg, projection) # TODO: validity check, should clock if the relationships arent valid.\n",
        "\n",
        "# Confirm that this is the projection you intended:\n",
        "sfm.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NATPdYARTaaq"
      },
      "source": [
        "# Run Fairness Tasks\n",
        "\n",
        "(This will have a lot of the same tasks & suggestions as Drago's git thinggy https://dplecko.github.io/CFA/)\n",
        "\n",
        "Most fairness analysis can be split into three general tasks: (1) bias detection and quantification, (2) fair prediction, and (3) fair decision-making. (***TODO***: elaborate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Bias Detection & Quantification\n",
        "Here we will evaluate \"fairness measures\" which can determine whether discrimination is present within a dataset, and indicate how strong that discrimination is.\n",
        "\n",
        "1. *Direct Effect (DE)* indicates whether your protected attribute *X* is directly impacting your outcome variable *Y*. \n",
        "\n",
        "2. *Indirect Effect (IE)* indicates whether your outcome variable is being indirectly impacted by your protected attribute. \n",
        "\n",
        "For example, if *X* denotes race and *Y* denotes salary, \"direct effect\" would indicate whether two identical candidates of different races would be given the same salary, whereas \"indirect effect\" would indicate that a candidate's race impacts some other attribute which in turn impacts salary. For example, if race causes educational discrimination, and salary is impacted by a candidate's degree level, then race would indirectly impact salary. \n",
        "\n",
        "3. *Spurious Effect (SE)* indicates whether there are variables that causally affect both your outcome variable and protected attribute, causing them to be correlated. \n",
        "\n",
        "For example, race causally affects an individual's hair color (if they are not of European descent, then there is an incredibly high likelihood that they will have black hair). If race also affects salary (directly or indirectly), then there will be a spurious effect of hair color on salary, even though hair color itself does not directly or indirectly impact a candidate's salary. \n",
        "\n",
        "The *Total Variation (TV)*, a measure of how much the distribution of *Y* is impacted by the value of a given attribute, can be calculated from the direct, indirect, and spurious effects. \n",
        "\n",
        "The `fairness_cookbook` below will calculate and store bias detection information. Once it is defined, you can use it to print and plot bias detection data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create your cookbook\n",
        "fcb = fairness_cookbook(data, sfm.X, sfm.W, sfm.Z, sfm.Y, x0=0, x1=1)\n",
        "\n",
        "# Get your data:\n",
        "autoplot(fcb, decompose='xspec', dataset=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Fair Prediction \n",
        "The goal of \"Fair Prediction\" tasks is to construct a predictor $\\hat{Y}$ of $Y$, which is also a function of $X$, $Z$, and $W$, but does not carry over any bias from the existing data. This is achieved by ensuring $\\hat{Y}$ satisfies some kind of \"fairness constraint\". \n",
        "\n",
        "These predictors don't always have to be \"fair\" with respect to every single attribute. For example, say a hospital is building a predictor to scan through applications and predict whether the candidate will be a good Doctor. They will likely include education status as a mediator variable, however the predictor should not be \"fair\" with respect to educational status. While gender bias and racial discrimination should be eliminated from the model, the hospital needs to maintain a strict bias against anyone who has not completed medical school. This is what's known as a \"Business Necessity\".\n",
        "\n",
        "When coming up with fair predictors, we will need to first define a \"Business Necessity\" set, which in our case just refers to the set of variables for which bias should not be eliminated. When using the standard fairness model (which we are), the allowed business necessity sets are: {$Z$}, {$W$}, {$Z,W$}, or none."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "fair_pred = fair_predictions(data, sfm, x0=0, x1=1, bn='zw')\n",
        "\n",
        "# You can now obtain predictions on new data\n",
        "preds = predict(fair_pred, data)\n",
        "data['fair_predictions'] = preds\n",
        "\n",
        "# And decompose the predictions on the evaluation set\n",
        "faircause_decomposition = fairness_cookbook(data, sfm.X, sfm.W, sfm.Z, 'fair_predictions', x0=0, x1=1)\n",
        "# You can now run the same quantification that you did with the cookbook under task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Fair Decision-Making\n",
        "Fair decision-making relates more to a group's well-being over time. We might be interested in creating a policy $\\pi$ so that the observed probability distribution \"improves\" over time, per that policy. For example if the observed distribution at time $t$ is $P_t(V)$, then we would want to design a policy such that $P_{t+1}(V)=\\pi(P_t(V))$. \n",
        "\n",
        "Concepts like affirmative action fall into the fair decision-making category. [note: everything up to this point has been rewording textbook ch 6.4 pg 404 task 3 desc.]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "resp_oc = fair_decisions(data, sfm, x0=0, x1=1, po_transform='', po_diff_sign=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now analyze important aspects of decision-making. Options for 'type' include:\n",
        "* \"decision\": decomposition of D\n",
        "* \"delta\": decomposition of Delta\n",
        "* \"benefit_fairness\": inspect benefit fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze important aspects of decision-making\n",
        "autoplot(resp_oc, type = \"decision\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOoTiWA3cC9RHRSH13/DGw1",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
