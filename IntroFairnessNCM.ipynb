{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhaller0826/ci2-fairness-ncm/blob/main/IntroFairnessNCM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgunRRxRSECt"
      },
      "source": [
        "# Introduction/Explanation\n",
        "\n",
        "Causal Inference [***TODO***: briefly explain the field of study & purpose of the notebook, especially as it relates to non-causal researchers]\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "**Section 1: Create a Causal Graph** \n",
        "This will walk you through creating a graph to represent relationships between the variables you plan to analyze. \n",
        "\n",
        "**Section 2: Train the Model**\n",
        "This will walk you through creating a neural causal model, and training it to learn the relationships that are represented in your causal graph based on your data.\n",
        "\n",
        "**Section 3: Extract Causal Insights from the Model**\n",
        "This will provide instructions for extracting information about the causal relationships in your trained model. This may contain useful tools for your analysis, but does not contain any steps that are necessary for progressing to Section 4.\n",
        "\n",
        "**Section 4: Project onto the Standard Fairness Model**\n",
        "This will walk you through projecting your trained model onto a standard fairness model, which can then be used to perform fairness analysis. Note that the tools for extracting metrics described in Section 3 can also be applied to the resulting standard fairness model. \n",
        "\n",
        "**Section 5: Run Fairness Tasks** \n",
        "This will walk you through using your Standard Fairness Model projection to perform bias detection, fair prediction, and fair decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell before you progress to ensure this notebook has access to the necessary code.\n",
        "import pandas as pd\n",
        "from src.utilities import *\n",
        "from src.graph.utils import *\n",
        "from src.graph.causal_graph import CausalGraph\n",
        "from src.training.train import *\n",
        "from src.metric.probabilities import ReusableProbability\n",
        "from src.metric.queries import *\n",
        "from src.model.sfm import SFM\n",
        "from src.fairness.task1 import FairnessCookbook\n",
        "from src.fairness.task2 import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj1bp-5ESFoq"
      },
      "source": [
        "# Create a Causal Graph\n",
        "\n",
        "A causal graph (or \"causal diagram\") represents the relationships between the features in your data. Each node on the graph may represent one or more of these features, though the name of the node does *not* need to correspond to the data column that it will represent. Please note that if a single node represents multiple features, we will not be able to distinguish between these features when doing causal analysis. \n",
        "\n",
        "If the values in one node may impact the values in another node, the graph will have a directed edge from the first node to the one that it affects. \n",
        "\n",
        "Sometimes there are confounders between two variables (lets call them A and B), meaning they are both directly impacted by the same third variable C. If C is one of the nodes on your graph, then there will be one directed edge from C to A, and another from C to B. If C is *not* one of the nodes on your graph, then we will create a bi-directed edge between A and B to signify that they are confounded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples of some graphs:\n",
        "\n",
        "![](img/default_graphs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define your graph below.\n",
        "\n",
        "You can use one of the pre-defined graphs, like so:\n",
        "```\n",
        "my_graph = get_predefined_graph(type='____')\n",
        "```\n",
        "Valid types include `simple`, `bow`, `frontdoor`, `backdoor`, `napkin`, and `sfm`. \n",
        "\n",
        "\n",
        "Alternatively, you create your own graph. Here is an example:\n",
        "```\n",
        "nodes = ['W', 'X', 'Y', 'Z']\n",
        "edges = [('X', 'Y'),\n",
        "        ('X', 'W'),\n",
        "        ('Z', 'Y'),\n",
        "        ('Z', 'W'),\n",
        "        ('W', 'Y'),\n",
        "        ('X', 'Z', 'bidirected'),\n",
        "        ('Z', 'Y', 'bidirected')]\n",
        "\n",
        "my_graph = CausalGraph(nodes=nodes, edges=edges)\n",
        "```\n",
        "\n",
        "*Remember*: when defining the directed, the arrow goes from the first node to the second one. So for example, ```edges = [('X', 'Y')]``` indicates one edge going from 'X' to 'Y'. When defining a bidirected edge, it does not matter which direction nodes are listed, but you must add the key term `'bidirected'` after listing the two nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION 1 -- using a pre-defined graph:\n",
        "my_graph = get_predefined_graph('backdoor')\n",
        "\n",
        "# OPTION 2 -- using a custom graph (delete this if you pick option 1): \n",
        "nodes = ['A', 'B', 'C', 'D', 'E', 'F', 'G'] # Node names do not need to correspond to the names of your data columns. \n",
        "bidirected_edges = [\n",
        "    ('C', 'A'),\n",
        "    ('F', 'A'),\n",
        "]\n",
        "directed_edges = [\n",
        "    ('A', 'B'),\n",
        "    ('A', 'D'),\n",
        "    ('A', 'E'),\n",
        "    ('A', 'G'),\n",
        "    ('C', 'B'),\n",
        "    ('C', 'D'),\n",
        "    ('C', 'E'),\n",
        "    ('C', 'G'),\n",
        "    ('F', 'B'),\n",
        "    ('F', 'D'),\n",
        "    ('F', 'E'),\n",
        "    ('F', 'G'),\n",
        "    ('B', 'G'),\n",
        "    ('D', 'G'),\n",
        "    ('E', 'G'),\n",
        "]\n",
        "\n",
        "my_graph = CausalGraph(nodes, directed_edges, bidirected_edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Double-check that this is the graph you want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
              " -->\n",
              "<!-- Title: G Pages: 1 -->\n",
              "<svg width=\"267pt\" height=\"255pt\"\n",
              " viewBox=\"0.00 0.00 267.30 254.58\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 250.58)\">\n",
              "<title>G</title>\n",
              "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-250.58 263.3,-250.58 263.3,4 -4,4\"/>\n",
              "<!-- F -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>F</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"232.3\" cy=\"-123.29\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"232.3\" y=\"-118.24\" font-family=\"Times,serif\" font-size=\"14.00\">F</text>\n",
              "</g>\n",
              "<!-- A -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>A</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"100.27\" cy=\"-228.58\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"100.27\" y=\"-223.53\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
              "</g>\n",
              "<!-- F&#45;&gt;A -->\n",
              "<g id=\"edge17\" class=\"edge\">\n",
              "<title>F&#45;&gt;A</title>\n",
              "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M205.78,-144.44C182.8,-162.77 149.71,-189.16 126.74,-207.47\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"207.95,-147.19 213.59,-138.22 203.59,-141.72 207.95,-147.19\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"124.7,-204.63 119.06,-213.6 129.06,-210.1 124.7,-204.63\"/>\n",
              "</g>\n",
              "<!-- E -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>E</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-170.15\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"27\" y=\"-165.1\" font-family=\"Times,serif\" font-size=\"14.00\">E</text>\n",
              "</g>\n",
              "<!-- F&#45;&gt;E -->\n",
              "<g id=\"edge11\" class=\"edge\">\n",
              "<title>F&#45;&gt;E</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M206.3,-129.23C170.28,-137.45 105.01,-152.35 63.94,-161.72\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"63.24,-158.29 54.27,-163.93 64.79,-165.12 63.24,-158.29\"/>\n",
              "</g>\n",
              "<!-- D -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>D</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-76.43\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"27\" y=\"-71.38\" font-family=\"Times,serif\" font-size=\"14.00\">D</text>\n",
              "</g>\n",
              "<!-- F&#45;&gt;D -->\n",
              "<g id=\"edge10\" class=\"edge\">\n",
              "<title>F&#45;&gt;D</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M206.3,-117.36C170.28,-109.14 105.01,-94.24 63.94,-84.86\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"64.79,-81.47 54.27,-82.66 63.24,-88.29 64.79,-81.47\"/>\n",
              "</g>\n",
              "<!-- B -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>B</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"100.27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"100.27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n",
              "</g>\n",
              "<!-- F&#45;&gt;B -->\n",
              "<g id=\"edge9\" class=\"edge\">\n",
              "<title>F&#45;&gt;B</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M214.77,-109.31C192.28,-91.37 153.06,-60.09 126.88,-39.22\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"129.06,-36.48 119.06,-32.98 124.7,-41.95 129.06,-36.48\"/>\n",
              "</g>\n",
              "<!-- G -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>G</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"191.64\" cy=\"-38.85\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"191.64\" y=\"-33.8\" font-family=\"Times,serif\" font-size=\"14.00\">G</text>\n",
              "</g>\n",
              "<!-- F&#45;&gt;G -->\n",
              "<g id=\"edge12\" class=\"edge\">\n",
              "<title>F&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.88,-105.8C218.43,-94.48 211.21,-79.49 204.99,-66.57\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"208.21,-65.18 200.71,-57.69 201.9,-68.22 208.21,-65.18\"/>\n",
              "</g>\n",
              "<!-- C -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>C</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"191.64\" cy=\"-207.73\" rx=\"27\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"191.64\" y=\"-202.68\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
              "</g>\n",
              "<!-- C&#45;&gt;A -->\n",
              "<g id=\"edge16\" class=\"edge\">\n",
              "<title>C&#45;&gt;A</title>\n",
              "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M154.51,-216.2C148.89,-217.49 143.09,-218.81 137.46,-220.1\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"155.26,-219.62 164.23,-213.99 153.7,-212.8 155.26,-219.62\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"136.72,-216.68 127.75,-222.31 138.28,-223.5 136.72,-216.68\"/>\n",
              "</g>\n",
              "<!-- C&#45;&gt;E -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>C&#45;&gt;E</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M165.92,-201.86C138.57,-195.62 95.07,-185.69 64.04,-178.61\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"64.82,-175.19 54.29,-176.38 63.26,-182.02 64.82,-175.19\"/>\n",
              "</g>\n",
              "<!-- C&#45;&gt;D -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>C&#45;&gt;D</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M174.02,-193.68C145.06,-170.58 87.22,-124.46 53.36,-97.46\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"55.68,-94.83 45.68,-91.33 51.31,-100.3 55.68,-94.83\"/>\n",
              "</g>\n",
              "<!-- C&#45;&gt;B -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>C&#45;&gt;B</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M183.21,-190.22C167.3,-157.18 132.74,-85.41 113.63,-45.75\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"116.83,-44.32 109.34,-36.83 110.53,-47.36 116.83,-44.32\"/>\n",
              "</g>\n",
              "<!-- C&#45;&gt;G -->\n",
              "<g id=\"edge8\" class=\"edge\">\n",
              "<title>C&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M191.64,-189.34C191.64,-160.28 191.64,-103.47 191.64,-68.58\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"195.14,-68.74 191.64,-58.74 188.14,-68.74 195.14,-68.74\"/>\n",
              "</g>\n",
              "<!-- A&#45;&gt;E -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>A&#45;&gt;E</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M82.91,-214.74C74.12,-207.73 63.27,-199.07 53.47,-191.26\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"55.68,-188.54 45.68,-185.05 51.31,-194.02 55.68,-188.54\"/>\n",
              "</g>\n",
              "<!-- A&#45;&gt;D -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>A&#45;&gt;D</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M91.87,-211.13C79.22,-184.88 55.2,-134.99 40.2,-103.85\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"43.46,-102.54 35.96,-95.05 37.15,-105.57 43.46,-102.54\"/>\n",
              "</g>\n",
              "<!-- A&#45;&gt;B -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>A&#45;&gt;B</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M100.27,-210.26C100.27,-173.86 100.27,-92.12 100.27,-47.85\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"103.77,-47.94 100.27,-37.94 96.77,-47.94 103.77,-47.94\"/>\n",
              "</g>\n",
              "<!-- A&#45;&gt;G -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>A&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M108.7,-211.08C124.61,-178.04 159.18,-106.27 178.28,-66.6\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"181.39,-68.21 182.57,-57.69 175.08,-65.18 181.39,-68.21\"/>\n",
              "</g>\n",
              "<!-- E&#45;&gt;G -->\n",
              "<g id=\"edge15\" class=\"edge\">\n",
              "<title>E&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M44.62,-156.1C73.59,-133 131.42,-86.88 165.28,-59.88\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"167.33,-62.72 172.97,-53.75 162.96,-57.25 167.33,-62.72\"/>\n",
              "</g>\n",
              "<!-- D&#45;&gt;G -->\n",
              "<g id=\"edge14\" class=\"edge\">\n",
              "<title>D&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M52.73,-70.56C80.07,-64.32 123.57,-54.39 154.6,-47.31\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"155.38,-50.72 164.35,-45.08 153.82,-43.9 155.38,-50.72\"/>\n",
              "</g>\n",
              "<!-- B&#45;&gt;G -->\n",
              "<g id=\"edge13\" class=\"edge\">\n",
              "<title>B&#45;&gt;G</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M126.21,-23.92C134.99,-25.92 144.97,-28.2 154.44,-30.36\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"153.63,-33.77 164.16,-32.58 155.19,-26.95 153.63,-33.77\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<graphviz.sources.Source at 0x126b1c350>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_graph.plot(scale=1.5)\n",
        "# Note: Currently, if you add too many nodes with long names, it may throw an error.\n",
        "# In this case, try reducing the names of the nodes to 3 characters or less, or reduce the number of nodes in your graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTCqyhX8S4s4"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete the Structural Causal Model\n",
        "Now that you have defined the graph we have the variables and dependencies, but we still need to learn the relationships between these variables to understand the causal mechanisms at work.\n",
        "\n",
        "These relationships will be learned based on your data, so we will start by processing your data file. This happens in the following order:\n",
        "1. Load your data as a `DataFrame` object. If it is a csv file, you can just replace the filename in the code below and your data will be loaded properly. \n",
        "2. Once loaded, you can alter the data if needed before continuing with processing. The default processing functions will handle converting categorical data and other standard tasks, but if, for example, you want one of the features to be converted into binary value, you will have to do that here.\n",
        "3. Specify which features in your data have categorical values, and which have discrete values. The default data processing will assume that all discrete variables can be rounded to whole numbers. \n",
        "4. **Assign features**: As stated above, the nodes of your graph need not correspond to the corresponding data features. You will instead manually define that correspondance here, in the following format: \n",
        "    ```\n",
        "    assignments = {\n",
        "        'node_1': ['data_feature', 'data_feature', ...],\n",
        "        'node_2': ['data_feature', 'data_feature', ...],\n",
        "        ...\n",
        "    }\n",
        "    ```\n",
        "    If a node only has one corresponding feature, you would assign it like so: `'node': ['data_feature']`.\n",
        "\n",
        "    Assignments have the following restrictions:\n",
        "    * All nodes you assign must be present in the graph above, and every node in the graph must have an assignment. Empty assignments will not be accepted. \n",
        "    * Each data feature can only be assigned to a single node. \n",
        "    * Each data feature *must* be present in the `DataFrame` object you created in step (1). \n",
        "    It is ok if some of your data features are excluded from the model, but any excluded features will not be used in the subsequent causal analysis. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is okay to exclude features from the model but they will not be used in the causal analysis.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Hanita/causal/ci2-fairness-ncm/src/utilities.py:81: UserWarning: The following features were not assigned to any variable: {'is_violent_recid', 'age_cat', 'vr_offense_date', 'v_screening_date', 'c_charge_degree', 'v_type_of_assessment', 'days_b_screening_arrest', 'id', 'decile_score', 'decile_score.1', 'c_case_number', 'c_days_from_compas', 'dob', 'r_case_number', 'v_decile_score', 'r_charge_desc', 'r_offense_date', 'type_of_assessment', 'name', 'screening_date', 'r_days_from_arrest', 'vr_charge_desc', 'v_score_text', 'first', 'r_jail_in', 'vr_charge_degree', 'c_jail_in', 'c_offense_date', 'compas_screening_date', 'c_charge_desc', 'num_r_cases', 'num_vr_cases', 'vr_case_number', 'score_text', 'r_jail_out', 'last', 'c_arrest_date', 'c_jail_out'}\n",
            "  warnings.warn('The following features were not assigned to any variable: {}'.format(unassigned_features), UserWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>race</th>\n",
              "      <th>juv_fel_count</th>\n",
              "      <th>juv_misd_count</th>\n",
              "      <th>juv_other_count</th>\n",
              "      <th>age</th>\n",
              "      <th>priors_count</th>\n",
              "      <th>r_charge_degree</th>\n",
              "      <th>sex</th>\n",
              "      <th>is_recid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6107</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141026</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6463</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6560</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4003</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115385</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      race  juv_fel_count  juv_misd_count  juv_other_count       age  \\\n",
              "6107   0.0            0.0             0.0         0.000000  0.141026   \n",
              "6463   0.0            0.0             0.0         0.000000  0.205128   \n",
              "6560   1.0            0.0             0.0         0.176471  0.012821   \n",
              "1396   0.0            0.0             0.0         0.000000  0.576923   \n",
              "4003   0.0            0.0             0.0         0.000000  0.115385   \n",
              "\n",
              "      priors_count  r_charge_degree  sex  is_recid  \n",
              "6107      0.000000              0.5  1.0       1.0  \n",
              "6463      0.046512              1.0  1.0       0.5  \n",
              "6560      0.000000              1.0  1.0       0.5  \n",
              "1396      0.069767              1.0  1.0       0.5  \n",
              "4003      0.023256              1.0  0.0       0.5  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the CSV file with your data:\n",
        "df = pd.read_csv('data/compas-scores.csv')\n",
        "\n",
        "# Do any special pre-processing that you may want to do (this is optional):\n",
        "df['race'] = df['race'].apply(lambda x: int(x=='Caucasian'))\n",
        "\n",
        "# Specify which features in your data are categorical, and which are discrete. \n",
        "categorical = ['race', 'age_cat', 'r_charge_degree', 'sex', 'score_text', 'is_recid']\n",
        "discrete = ['juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n",
        "\n",
        "# Now define which columns of your data correspond to each node in your graph:\n",
        "assignments = {\n",
        "    'A': ['race'],\n",
        "    'B': ['juv_fel_count', 'juv_misd_count', 'juv_other_count'],\n",
        "    'C': ['age'],\n",
        "    'D': ['priors_count'],\n",
        "    'E': ['r_charge_degree'],\n",
        "    'F': ['sex'],\n",
        "    'G': ['is_recid']\n",
        "}\n",
        "\n",
        "# Process data & assignments\n",
        "my_data = process_data_assignments(df, assignments, my_graph, categorical, discrete)\n",
        "my_data.print_df()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***TODO***: fill out model instructions. This will vary depending on whether I am able to implement more than just FF_NCM. \n",
        "\n",
        "***TODO***: multiple distribution options. \n",
        "\n",
        "When training the network, you are able to manually adjust various settings called \"hyperparameters\". These include the following:\n",
        "* `pipeline_choice`: this specifies the type of neural network that will be ran. The options are as follows,\n",
        "    * `'mle'`: Maximum Likelihood Estimator\n",
        "    * `'gan'`: Generative Adversarial Network\n",
        "    * `'ffn'`: Feed Forward Network\n",
        "    The default is 'mle'. \n",
        "* `n-epochs`: the number of epochs, or passes through the dataset to complete during the training phase, to run. Default: 1000. \n",
        "* `batch-size`: number of training examples to use in each training epoch. \n",
        "* `gpu`: the GPU to use, if you have one. Default: none\n",
        "\n",
        "You may set some, none, or all of these values manually. Valid definitions include:\n",
        "```\n",
        "hyperparameters = {\n",
        "    'pipeline_choice': 'gan',\n",
        "    'n-epochs': 500\n",
        "}\n",
        "```\n",
        "```\n",
        "hyperparameters = {}\n",
        "```\n",
        "```\n",
        "hyperparameters = {\n",
        "    'pipeline_choice': 'ffn',\n",
        "    'n-epochs': 4600,\n",
        "    'batch-size': 4,\n",
        "    'gpu': gpu\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 0.1352\n",
            "Epoch 2/2, Loss: 0.0896\n"
          ]
        }
      ],
      "source": [
        "# Set your parameters here: # TODO\n",
        "hyperparameters = {\n",
        "    'model_choice': 'gan',\n",
        "    'distribution': 'uniform',\n",
        "    'nn': 'mlp',\n",
        "    'device': 'cpu',\n",
        "    'n-epochs': 2,\n",
        "    'learning-rate': 1e-3,\n",
        "    'optimizer': None\n",
        "}\n",
        "\n",
        "# And train the model:\n",
        "my_model = get_ncm(my_graph, assignments, scale=my_data.get_assigned_scale())\n",
        "_ = train_ncm(my_model, my_data, hyperparameters=hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYOhC9bCTRMh"
      },
      "source": [
        "# Extract Causal Insights from the Model\n",
        "Now that you have a trained model, you can use it to evaluate cause-effect relationships in your data. The metrics that you can evaluate are split into three categories: (1) values that can be computed directly from the data, (2) values that would occur if you force a variable to take on a certain value, and (3) questions about what would have happened in an imaginary world where some observed facts are bluntly negated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observational Understanding\n",
        "Observational metrics can be calculated directly from the data. These include calculating standard probabilities and conditional probabilities (ex.: $P(Y=1)$ or $P(Y=1|X=0)$), as well as something called the \"total variation\".\n",
        "\n",
        "*Total Variation (TV)*: a measure of how much the probability distribution of some variable $Y$ is impacted by the observed value of a given attribute $X$. If possible values of $X$ are $x_0, x_1$ then the Total Variation can be calculated by the equation below.\n",
        "$$TV_{x_0,x_1}(y) = P(Y=y|X=x_1)-P(Y=y|X=x_0)$$\n",
        "\n",
        "The available observational measures are defined as follows:\n",
        "* `probability(model, variable, value, evidence)`: This will calculate the probability that a variable equals a certain value given the dictionary of evidence. The dictionary of conditional evidence is optional.\n",
        "    * `probability(model, 'Y',1)` will return $P(Y=1)$\n",
        "    * `probability(model, 'Y',1,{'X':0,'Z':2})` will return $P(Y=1|X=0,Z=2)$\n",
        "* `total_variation(model, variable, value, attr, aval0, aval1)` will return the total variation of the given variable relative to the given attribute. $TV_{aval0,aval1}(variable=value)$. `avail1` is an optional value, and will default to \"not avail0\" if left blank. \n",
        "    * `total_variation(model,'Y',1,'X',x0,x1)` will return $TV_{x_0,x_1}(Y=1) = P(Y=1|X=x_1)-P(Y=1|X=x_0)$\n",
        "    * `total_variation(model,'Y',1,'X',x0)` will return $TV_{x_0,x_1}(Y=1) = P(Y=1|X=x_1)-P(Y=1|X\\neq x_1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TV(G=1) = 0.3382 - 0.3604 = -0.0222\n",
            "TV(G=1) = 0.3387 - 0.3741 = -0.0355\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-0.03548100994755232"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob = ReusableProbability(my_model, 'G', 'A', 1)\n",
        "prob.total_variation(1)\n",
        "total_variation(my_model, 'G', 1, 'A', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interventional Metrics\n",
        "Interventional metrics arise when you actively say \"what if we force this variable to take on a certain value?\"\n",
        "The difference between a conditional probability ($P(Y|X=x)$) and an interventional one ($P(Y|do(X=x))$) is that when you force an event to happen, you emancipate it from all other influences. [kinda stole this wording from the book of why but not exactly...]\n",
        "\n",
        "*For example*: say that patients who go to the doctor for a certain illness all recieve advice and prescription medication, and this allows them to heal within a week. Say that those who have not recieved the advice nor the medication don't heal so quickly.\n",
        "\n",
        "![](img/dr_advice_meds_healed.png)\n",
        "\n",
        "If we are trying to evaluate the likelihood that someone has healed, and we observe that they have recieved the advice, we can assume from our data that they went to the doctor and recieved medication too. \n",
        "\n",
        "Essentially, {patient got advice} $\\implies$ {patient went to doctor} $\\implies$ {patient got advice} AND {patient got medication}\n",
        "\n",
        "This means that $P(\\text{healed} | \\text{advice})=P(\\text{healed} | \\text{advice},\\text{medication})$.\n",
        "\n",
        "But what if a patient hears the doctor's advice from a friend? In this case, even though the patient has heard the advice, they may not have seen the doctor, and they may not have access to the medication. This is an example of an intervention. Although we can usually assume that someone who has heard the advice has seen a doctor, the friend intervened and gave the patient advice, so even though we know the patient has heard the advice, we don't have any knowledge of whether or not they went to a doctor and recieved medication. Here, $P(\\text{healed} | do(\\text{advice}))\\neq P(\\text{healed} | \\text{advice})$.\n",
        "\n",
        "*Total Effect (TE)*: Similar to Total Variation, except these variations represent changes in behavior based on the value of $X$, rather than comparing the naturally arising distribution based on $X$. \n",
        "\n",
        "$$TE_{x_0,x_1}(y) = P(Y=y | do(X=x_1)) - P(Y=y | do(X=x_0))$$\n",
        "\n",
        "*z-Total Effect (z-TE)*: Total Effect with an additional observational condition. We intervene to force $X=\\{some-value\\}$, and we add the condition that some other variable $Z$ was observed to be $z$. \n",
        "\n",
        "$$z\\text{-}TE_{x_0,x_1}(y) = P(Y=y | do(X=x_1), Z=z) - P(Y=y | do(X=x_0), Z=z)$$\n",
        "\n",
        "The available interventional measures are defined as follows:\n",
        "* `probability(model,variable, value, evidence, intervention)`: This will calculate the probability that a variable equals a certain value given the dictionary of evidence, and the dictionary of interventions. Both dictionaries are optional.\n",
        "    * `probability(model,'Y',1,{'X':0},{'Z':1})` will return $P(Y=1 | X=0, do(Z=1))$\n",
        "    * `probability(model,'Y',1,intervention={'Z':1})` will return $P(Y=1 | do(Z=1))$\n",
        "* `total_effect(model, variable, value, attr, aval1, aval0, evidence)` \n",
        "    * `total_effect(model, 'Y', 1, 'X', 1, 0)` will return $P(Y=1 | do(X=1)) - P(Y=1 | do(X=0)) = TE_{0,1}(y)$\n",
        "    * `total_effect(model, 'Y', 1, 'X', 1, 0, evidence={Z:z})` will return $P(Y=1 | do(X=1),Z=z) - P(Y=1 | do(X=0),Z=z) = z$-$TE_{0,1}(y)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TE(G=1) = 0.3290 - 0.3624 = -0.0334\n",
            "TE(G=1) = 0.3379 - 0.3738 = -0.0359\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-0.03590000000000004"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob.total_effect(1, x1=1, x0=0)\n",
        "total_effect(my_model, 'G', 1, 'A', 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What-ifs\n",
        "Lastly we can evaluate \"counterfactuals\", named because it is the probability of something that we have no real world data on. \n",
        "\n",
        "For example, say people have the option to take a medication or heal naturally, and that the observational data shows that individuals who healed naturally actually survived longer on average. Is this because the medication was harmful, or is it because the *types* of people who decided to heal naturally were typically younger, healthier, had fewer comorbidities, or had fewer symptoms to begin with? \n",
        "\n",
        "What is the probability that someone who opted out of medication would have healed had they taken it? What is the probability that someone who decided to take medication would have healed if they never got it? \n",
        "\n",
        "These are \"counterfactual\" questions because to answer them we would have to go back in time and change history. There is no experiment that can deny treatment to an already treated person and compare the two outcomes. \n",
        "\n",
        "#### New notation: $P(Y_X)$\n",
        "\n",
        "$P(Y_{X=x})$ is the same as saying \"probability of $Y$ intervened on to force $X=x$\". In the section above, this was denoted $P(Y|do(X=x))$. However when written with this notation, we are stating that the intervention $do(X=x)$ only applies to $Y$. \n",
        "\n",
        "If another variable in the model, $Z$, is also affected by $X$, then when calculating $P(Y|do(X=x),Z=z)$, the intervention $X=x$ will be applied to both $Y$ and $Z$. Conversely, when evaluating the term $P(Y_{X=x}|Z=z)$, the intervention is only applied to $Y$, and $Z$ will be calculated based on the unaffected/real $X$ values. \n",
        "\n",
        "#### Counterfactual Quantities:\n",
        "\n",
        "*Effect of Treatment on the Treated (ETT)*: evaluate whether people who gained access to treatment would have had the same outcome if they had never recieved it.\n",
        "$$ETT_{x,x'}(Y)=P(Y_{X=x}=y | X=x')$$\n",
        "\n",
        "*Probability of Necessity (PN)*: encodes how much the presence of a value $X$ was a *necessary* cause to make $Y=1$.\n",
        "$$PN(X;Y)=P(Y_{X=0}=0|X=1,Y=1)$$\n",
        "\n",
        "*Probability of Sufficiency (PS)*: encodes how much the presence of a value $X$ was *sufficient* to make $Y=1$.\n",
        "$$PS(X;Y)=P(Y_{X=1}=1|X=0,Y=0)$$\n",
        "\n",
        "Both of the above translate to \"in a situation where $X=x$ and $Y=y$, what is the probability that the value of $Y$ would have been different if you had changed the value of $X$?\" These can be combined into \n",
        "$$PN/PS_{(x,y)(x',y')}(X;Y) = P(Y_{X=x}=y | X=x', Y=y')$$\n",
        "\n",
        "\n",
        "The available counterfactual measures are defined as follows:\n",
        "* `ett(outcome_var, outcome_val, treatment_var, treatment_vals={'actual':_, 'whatif':_})`: This will calculate the probability of a given outcome if we had intervened to change the treatment. In the `treatment_vals` dictionary, you must specify either `actual` or `whatif`, but you do not need to specify both. \n",
        "  * `ett('Y',1,'X',{'actual':0, 'whatif':1})` will return $P(Y_{X=1}=1|X=0)$, which is the probability that $Y=1$ if we had intervened to make $X=1$, given that $X$ was actually 0.\n",
        "  * `ett('Y',1,'X',{'whatif':1})` will return $P(Y_{X=1}=1|X\\neq 1)$, which is the probability that $Y=1$ if we had intervened to make $X=1$, given that $X$ was not actually 1.\n",
        "  * Note: the \"whatif\" value must be specified so `treatment_vals={'actual':_}` is not a valid input.\n",
        "\n",
        "* `pnps(outcome_var, outcome_vals={'actual':_, 'whatif':_}, treatment_var, treatment_vals={'actual':_, 'whatif':_})` \n",
        "  * `pnps('Y', outcome_vals={'actual':0, 'whatif':1}, 'X', treatment_vals={'actual':0, 'whatif':1})` will return the probability that $Y=1$ if we had intervened to make $X=1$, given that $Y$ and $X$ were actually both 0. \n",
        "  * Note: like with ETT, the \"whatif\" values must be explicitly stated in both `outcome_vals` and `treatment_vals`, but the \"actual\" values may be assumed. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0021: probability that G=1 if we had intervened to make A=1, given that G was not actually 1 and A was not actually 1.\n",
            "0.0011: probability that G=1 if we had intervened to make A=1, given that G was not actually 1 and A was not actually 1.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0011299435028248588"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outcome_vals = {\n",
        "    'actual': 0,\n",
        "    'whatif': 1\n",
        "}\n",
        "treatment_vals = {\n",
        "    'whatif': 1\n",
        "}\n",
        "prob.pnps(outcome_vals['whatif'], outcome_vals['actual'], treatment_vals['whatif'])\n",
        "pnps(my_model, 'G', outcome_vals, 'A', treatment_vals, assignments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3241: probability that G=1 if we had intervened to make A=1, given that A was not actually 1.\n",
            "0.3320: probability that G=1 if we had intervened to make A=1, given that A was not actually 1.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.33196072220462464"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob.ett(1, whatif_treatment=treatment_vals['whatif'])\n",
        "ett(my_model, 'G', 1, 'A', treatment_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tv0OBYGTF5U"
      },
      "source": [
        "# Project onto the Standard Fairness Model\n",
        "\n",
        "The standard fairness model (SFM) is a model whose graph has four nodes:\n",
        "* $X$: The protected attribute. When doing fairness analysis, we are trying to figure out whether or not an outcome is fair with respect to this attribute. (Ex.: race)\n",
        "* $Z$: The counfounding variables. These variables are not causally influenced by the protected attribute $X$, but may be correlated in some way. (Ex.: demographic information, zip code)\n",
        "* $W$: The mediator variables. Variables which may be causally influenced by the protected attribute. (Ex.: education-level, prior employment)\n",
        "* $Y$: The outcome variable. (Ex.: admissions, hiring decisions, salary)\n",
        "\n",
        "When projecting onto the SFM, you should select one of your model's variables to be $X$ and one to be $Y$, but you may assign multiple variables as confounders or mediators. Counfounders may have any relationship to the other confounders, and mediators may have any relationship with the other mediators, however there is a specific structure that must exist between $X$, $Z$, $W$, and $Y$:\n",
        "* $Y$ must be a variable that has 0 arrows pointing toward $X$, $Z$, or $W$.\n",
        "* $W$ must contain variables that have 0 arrows pointing toward $X$ or $Z$.\n",
        "* $X$ and $Z$ cannot have arrows pointing towards each other, but they may have a bidirected arrow between them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Here is an example**:\n",
        "Given the following graph. \n",
        "\n",
        "![example1graph.png](img/example1graph.png)\n",
        "\n",
        "Based on this graph, you could do either of the following projections:\n",
        "```\n",
        "projection1 = {\n",
        "    'X': 'e',\n",
        "    'Z': ['a', 'b'],\n",
        "    'W': ['c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "\n",
        "or \n",
        "```\n",
        "projection2 = {\n",
        "    'X': 'a',\n",
        "    'Z': ['e'],\n",
        "    'W': ['b', 'c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "\n",
        "![](img/ex1projection1.png) ![](img/ex1projection2.png)\n",
        "\n",
        "The following would **not** be a valid projection: \n",
        "```\n",
        "bad_projection = {\n",
        "    'X': 'a',\n",
        "    'Z': ['b', 'e'],\n",
        "    'W': ['c', 'd'],\n",
        "    'Y': 'f'\n",
        "}\n",
        "```\n",
        "because there is an arrow from the `X` variable (containing `'a'`) to the `Z` variable (containing `'b'`). This indicates that `X` may actually cause `Z`, rather than just being confounded with `Z`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Protected Attribute: ['race']\n",
            "Confounders:         ['age', 'sex']\n",
            "Mediators:           ['r_charge_degree', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count']\n",
            "Outcome:             ['is_recid']\n"
          ]
        }
      ],
      "source": [
        "# Specify your projection here:\n",
        "projection = {\n",
        "    'X': 'A',\n",
        "    'Z': ['C', 'F'],\n",
        "    'W': ['E','D','B'],\n",
        "    'Y': 'G'\n",
        "}\n",
        "\n",
        "sfm = SFM(my_model, projection, my_data) \n",
        "# Ensure this was the projection you were expecting:\n",
        "sfm.print_projection()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NATPdYARTaaq"
      },
      "source": [
        "# Run Fairness Tasks\n",
        "\n",
        "(This will have a lot of the same tasks & suggestions as Drago's git thinggy https://dplecko.github.io/CFA/)\n",
        "\n",
        "Most fairness analysis can be split into three general tasks: (1) bias detection and quantification, (2) fair prediction, and (3) fair decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Bias Detection & Quantification\n",
        "Here we will evaluate \"fairness measures\" which can determine whether discrimination is present within a dataset, and indicate how strong that discrimination is.\n",
        "\n",
        "1. *Direct Effect (DE)* indicates whether your protected attribute *X* is directly impacting your outcome variable *Y*. \n",
        "\n",
        "2. *Indirect Effect (IE)* indicates whether your outcome variable is being indirectly impacted by your protected attribute. \n",
        "\n",
        "For example, if *X* denotes race and *Y* denotes whether a candidate was hired, \"direct effect\" would indicate whether two identical candidates of different races would have the same likelihood of being hired, whereas \"indirect effect\" would indicate that a candidate's race impacts some other attribute (like education level, prior job experience, etc.) which in turn impacts salary.\n",
        "\n",
        "3. *Spurious Effect (SE)* indicates whether there are variables that causally affect both your outcome variable and protected attribute, causing them to be correlated. \n",
        "\n",
        "Race causally affects an individual's hair color (if they are not of European descent, then there is an incredibly high likelihood that they will have black hair). If race also affects whether a candidate is hired (directly or indirectly), then there will be a spurious effect of hair color on salary, even though hair color itself does not directly or indirectly impact a candidate's salary. \n",
        "\n",
        "When thinking about bias, we typically think about how some attribute $X$ impacts a decision or prediction $Y$. If the direct effect is non-zero, then there is evidence of disparate *treatment* in the outcome $Y$, meaning $X$ is being used in the decision process. \n",
        "\n",
        "If the indirect or spurious effects are non-zero, then there is evidence of disparate *impact*, which typically refers to cases where discrimination is unintended or implicit. Sometimes it is not possible to eliminate disparate impact. For example, if there is discrimination in the educational system, and a job requires a certain level of education, then they may hire more people of a certain demographic simply because more people from that demographic have the necessary qualifications. This is known as *business necessity*. \n",
        "\n",
        "The `fairness_cookbook` below will calculate bias detection information based on the Standard Fairness Model projection you created above. Note that you will need to define an $x_0$ and $x_1$ value, just like you did when calculating the $ETT$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xDE^sym = -0.10700.0365\t hypothesis H^xDE=0 REJECTED\n",
            "\t ---> evidence of disperate TREATMENT.\n",
            "xIE^sym = 0.00900.0365\t hypothesis H^xIE=0 ACCEPTED\n",
            "xSE_x1x0 = 0.02210.0309\t hypothesis H^xSE=0 ACCEPTED\n",
            "\t ---> no evidence of disperate IMPACT.\n",
            "\n",
            "xTE_x0x1(Y=1 | X=0) = -0.0355\n",
            "xDE^sym(Y=1 | X=0) = -0.0379\n",
            "xIE^sym(Y=1 | X=0) = 0.0024\n",
            "xSE_{'X': 1},{'X': 0}(Y=1) = -0.0258\n",
            "TV_{'X': 0},{'X': 1}(Y=1) = -0.0097\n"
          ]
        }
      ],
      "source": [
        "# create your cookbook\n",
        "fcb = FairnessCookbook(sfm=sfm, x0_val=0, x1_val=1)\n",
        "\n",
        "# Get your data:\n",
        "fcb.fairness_cookbook()\n",
        "fcb.x_specific_effects()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Fair Prediction \n",
        "The goal of \"Fair Prediction\" tasks is to construct a predictor $\\hat{Y}$ of $Y$, which is also a function of $X$, $Z$, and $W$, but does not carry over any bias from the existing data. This is achieved by ensuring $\\hat{Y}$ satisfies some kind of \"fairness constraint\". \n",
        "\n",
        "As stated above, these predictors don't always have to be \"fair\" with respect to every single attribute. For example, say a hospital is building a predictor to scan through applications and predict whether the candidate will be a good Doctor. They will likely include education status as a mediator variable, however the predictor should not be \"fair\" with respect to educational status. While gender bias and racial discrimination should be eliminated from the model, the hospital needs to maintain a strict bias against anyone who has not completed medical school. This is what's known as a \"Business Necessity\".\n",
        "\n",
        "When coming up with fair predictors, we will need to first define a \"Business Necessity\" set, which in our case just refers to the set of variables for which bias should not be eliminated. When using the standard fairness model (which we are), the allowed business necessity sets are: {$Z$}, {$W$}, {$Z,W$}, or none."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E 50, Train BCE: 0.01678, Causal Loss: 0.02141, Eval BCE: 0.01735,  Eval Causal: 0.01654, \n",
            "Restarting at E 84, Restart 1/5\n",
            "E 50, Train BCE: 0.02069, Causal Loss: 0.01572, Eval BCE: 0.01651,  Eval Causal: 0.01603, \n",
            "Restarting at E 93, Restart 2/5\n",
            "E 50, Train BCE: 0.02861, Causal Loss: 0.02417, Eval BCE: 0.02418,  Eval Causal: 0.01723, \n",
            "E 100, Train BCE: 0.02325, Causal Loss: 0.04771, Eval BCE: 0.01628,  Eval Causal: 0.01755, \n",
            "E 150, Train BCE: 0.01855, Causal Loss: 0.01472, Eval BCE: 0.01601,  Eval Causal: 0.0192, \n",
            "Restarting at E 178, Restart 3/5\n",
            "E 50, Train BCE: 0.01396, Causal Loss: 0.04244, Eval BCE: 0.01646,  Eval Causal: 0.02265, \n",
            "Restarting at E 92, Restart 4/5\n",
            "E 50, Train BCE: 0.02475, Causal Loss: 0.04174, Eval BCE: 0.02363,  Eval Causal: 0.01813, \n",
            "E 100, Train BCE: 0.01506, Causal Loss: 0.02765, Eval BCE: 0.01603,  Eval Causal: 0.01523, \n",
            "Restarting at E 145, Restart 5/5\n",
            "E 50, Train BCE: 0.01987, Causal Loss: 0.02699, Eval BCE: 0.01687,  Eval Causal: 0.02679, \n",
            "E 100, Train BCE: 0.02454, Causal Loss: 0.02991, Eval BCE: 0.01543,  Eval Causal: 0.01692, \n",
            "Restarting at E 140, Restart 6/5\n",
            "Max restarts reached or no early stop. Stopping.\n"
          ]
        }
      ],
      "source": [
        "x_col = sfm.assignments['X']\n",
        "z_cols = sfm.assignments['Z']\n",
        "w_cols = sfm.assignments['W']\n",
        "y_col = sfm.assignments['Y']\n",
        "\n",
        "lmbd = 0.5 # TODO : diff lambdas\n",
        "fair_pred = train_w_es(my_data.train_df, my_data.test_df, x_col, z_cols, w_cols, y_col, lmbd, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'keys'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fair_pred_from_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_w_es_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/causal/ci2-fairness-ncm/src/fairness/task2.py:301\u001b[0m, in \u001b[0;36mtrain_w_es_from_model\u001b[0;34m(model_scm, lmbd, n_samples, lr, epochs, patience, max_restarts, eta_de, eta_ie, eta_se_x1, eta_se_x0, relu_eps, eps, batch_size, verbose)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Estimate P(X=1 | Z) using FF_NCM's f['X'] directly\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 301\u001b[0m     px_z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(model_scm\u001b[38;5;241m.\u001b[39mf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]({ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m: Z }, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodel_scm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     px \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Construct feature matrix and indices\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/causal/ci2-fairness-ncm/src/model/ncm/mlp.py:43\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, pa, u, include_inp)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa, u, include_inp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m         inp \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mcat([pa[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pa\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(pa\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_pa)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
          ]
        }
      ],
      "source": [
        "fair_pred_from_model = train_w_es_from_model(sfm, lmbd, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'fair_predictions' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fair_pred \u001b[38;5;241m=\u001b[39m \u001b[43mfair_predictions\u001b[49m(data, sfm, x0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, x1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# You can now obtain predictions on new data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m preds \u001b[38;5;241m=\u001b[39m predict(fair_pred, data)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fair_predictions' is not defined"
          ]
        }
      ],
      "source": [
        "fair_pred = fair_predictions(data, sfm, x0=0, x1=1, bn='')\n",
        "\n",
        "# You can now obtain predictions on new data\n",
        "preds = predict(fair_pred, data)\n",
        "data['fair_predictions'] = preds\n",
        "\n",
        "# And decompose the predictions on the evaluation set\n",
        "faircause_decomposition = fairness_cookbook(data, sfm.X, sfm.W, sfm.Z, 'fair_predictions', x0=0, x1=1)\n",
        "# You can now run the same quantification that you did with the cookbook under task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Fair Decision-Making\n",
        "Fair decision-making relates more to a group's well-being over time. We might be interested in creating a policy $\\pi$ so that the observed probability distribution \"improves\" over time, per that policy. For example if the observed distribution at time $t$ is $P_t(V)$, then we would want to design a policy such that $P_{t+1}(V)=\\pi(P_t(V))$. \n",
        "\n",
        "Concepts like affirmative action fall into the fair decision-making category. [note: everything up to this point has been rewording textbook ch 6.4 pg 404 task 3 desc.]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "resp_oc = fair_decisions(data, sfm, x0=0, x1=1, po_transform='', po_diff_sign=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now analyze important aspects of decision-making. Options for 'type' include:\n",
        "* \"decision\": decomposition of D\n",
        "* \"delta\": decomposition of Delta\n",
        "* \"benefit_fairness\": inspect benefit fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze important aspects of decision-making\n",
        "autoplot(resp_oc, type = \"decision\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOoTiWA3cC9RHRSH13/DGw1",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
